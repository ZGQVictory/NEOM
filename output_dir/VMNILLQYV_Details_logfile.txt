We use, initially, the frequency matrix (20, 9) of the amino acids is:

[[0.09670933 0.04727767 0.09318748 0.06359184 0.06695064 0.05577483
  0.07373346 0.07539656 0.07912804]
 [0.05563581 0.02782904 0.03071733 0.03885578 0.05068845 0.03483082
  0.03975022 0.05933934 0.02761941]
 [0.03607488 0.02326393 0.04088249 0.04706901 0.04261546 0.04357512
  0.03828303 0.04518231 0.02293317]
 [0.02920283 0.0282059  0.06575365 0.10611515 0.0482236  0.03897642
  0.03821708 0.03277126 0.02727885]
 [0.01336861 0.01281424 0.0139649  0.01383446 0.01404409 0.01481741
  0.01393229 0.01372265 0.01395558]
 [0.03438036 0.03204644 0.04354369 0.04011967 0.04219272 0.03591302
  0.04418657 0.05779413 0.01794973]
 [0.0312741  0.02958772 0.03573232 0.12771499 0.06388847 0.04870632
  0.06506242 0.08554597 0.02779884]
 [0.06936174 0.03978939 0.0518643  0.07792412 0.06569548 0.04807226
  0.04065588 0.06370163 0.03801915]
 [0.02597335 0.01374937 0.02622025 0.02540035 0.03792714 0.02015019
  0.05140892 0.03268629 0.01365154]
 [0.06415993 0.07030919 0.07275026 0.04185489 0.06653112 0.10435839
  0.07044895 0.04111419 0.07752991]
 [0.08606066 0.39055614 0.13685261 0.06252577 0.09643987 0.14942132
  0.0923357  0.08309784 0.22522492]
 [0.08448639 0.02972072 0.03775668 0.05394507 0.07286336 0.04107821
  0.04591843 0.07359941 0.03068503]
 [0.01931519 0.04216062 0.0330858  0.01468927 0.0183136  0.01914748
  0.01929189 0.01802477 0.01773129]
 [0.06385644 0.02564252 0.05279709 0.02953239 0.04640559 0.03813204
  0.04944295 0.04158867 0.02922027]
 [0.02115105 0.02547882 0.03344957 0.06306384 0.03375237 0.05049979
  0.06891495 0.02737018 0.02077371]
 [0.0864455  0.03256961 0.06386095 0.06990772 0.05664489 0.0530485
  0.04890707 0.07507869 0.03093912]
 [0.05490157 0.03538698 0.04592924 0.04625999 0.05260025 0.0551112
  0.05606154 0.07437422 0.0307797 ]
 [0.00891878 0.00679915 0.02115674 0.00870449 0.01060983 0.00814546
  0.01490965 0.00824795 0.00704605]
 [0.04876444 0.02053375 0.04038375 0.0210229  0.03528265 0.02366428
  0.0385017  0.03857158 0.02032877]
 [0.06995904 0.0662788  0.0601109  0.04786829 0.07833041 0.11657694
  0.0900373  0.05279235 0.24140691]]

 Note:

column --> location

row --> amino acids, dict_keys(['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V'])

It consists of two parts, one is BLOSUM62, whose weight is 1.0. The other is IEDB, whose weight is 1.0
======================================================================

LOSS RESULTS of the dict_values(['V', 'M', 'N', 'I', 'L', 'L', 'Q', 'Y', 'V']):
****modeller loss --> 10 * 180.873444 = 1808.7344400000002
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * 5.41055 + -0.1 * -1267.67944
****cdr loss --> 100 * 61.1472165314073 = 6114.72165314073 given no cdr
****iedb loss --> 50 * 23.856770986018688 = 1192.8385493009343
----------------------------------------------------------------------------------------------------
Starting...
Loss ['V', 'M', 'N', 'I', 'L', 'L', 'Q', 'Y', 'V']: 9116.294642441664
Step 00000: change accepted >> LOSS inf --> 9116.295
sequence --> target:  ['V', 'M', 'N', 'I', 'L', 'L', 'Q', 'Y', 'V']
======================================================================
[[0.09670933 0.04727767 0.09318748 0.06359184 0.06695064 0.05577483
  0.07373346 0.07539656 0.07912804]
 [0.05563581 0.02782904 0.03071733 0.03885578 0.05068845 0.03483082
  0.03975022 0.05933934 0.02761941]
 [0.03607488 0.02326393 0.04088249 0.04706901 0.04261546 0.04357512
  0.03828303 0.04518231 0.02293317]
 [0.02920283 0.0282059  0.06575365 0.10611515 0.0482236  0.03897642
  0.03821708 0.03277126 0.02727885]
 [0.01336861 0.01281424 0.0139649  0.01383446 0.01404409 0.01481741
  0.01393229 0.01372265 0.01395558]
 [0.03438036 0.03204644 0.04354369 0.04011967 0.04219272 0.03591302
  0.04418657 0.05779413 0.01794973]
 [0.0312741  0.02958772 0.03573232 0.12771499 0.06388847 0.04870632
  0.06506242 0.08554597 0.02779884]
 [0.06936174 0.03978939 0.0518643  0.07792412 0.06569548 0.04807226
  0.04065588 0.06370163 0.03801915]
 [0.02597335 0.01374937 0.02622025 0.02540035 0.03792714 0.02015019
  0.05140892 0.03268629 0.01365154]
 [0.06415993 0.07030919 0.07275026 0.04185489 0.06653112 0.10435839
  0.07044895 0.04111419 0.07752991]
 [0.08606066 0.39055614 0.13685261 0.06252577 0.09643987 0.14942132
  0.0923357  0.08309784 0.22522492]
 [0.08448639 0.02972072 0.03775668 0.05394507 0.07286336 0.04107821
  0.04591843 0.07359941 0.03068503]
 [0.01931519 0.04216062 0.0330858  0.01468927 0.0183136  0.01914748
  0.01929189 0.01802477 0.01773129]
 [0.06385644 0.02564252 0.05279709 0.02953239 0.04640559 0.03813204
  0.04944295 0.04158867 0.02922027]
 [0.02115105 0.02547882 0.03344957 0.06306384 0.03375237 0.05049979
  0.06891495 0.02737018 0.02077371]
 [0.0864455  0.03256961 0.06386095 0.06990772 0.05664489 0.0530485
  0.04890707 0.07507869 0.03093912]
 [0.05490157 0.03538698 0.04592924 0.04625999 0.05260025 0.0551112
  0.05606154 0.07437422 0.0307797 ]
 [0.00891878 0.00679915 0.02115674 0.00870449 0.01060983 0.00814546
  0.01490965 0.00824795 0.00704605]
 [0.04876444 0.02053375 0.04038375 0.0210229  0.03528265 0.02366428
  0.0385017  0.03857158 0.02032877]
 [0.06995904 0.0662788  0.0601109  0.04786829 0.07833041 0.11657694
  0.0900373  0.05279235 0.24140691]]
Now the best target step is 0
########### Now Step-1 ###########
########### Mutation Positions are [8, 4] ###########

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'P', 'L', 'Q', 'Y', 'Y']:
****modeller loss --> 10 * 168.54733000000002 = 1685.4733
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * 4.99288 + -0.1 * -1186.1853
****cdr loss --> 100 * 103.14044761911984 = 10314.044761911984 given no cdr
****iedb loss --> 50 * 28.92414368409065 = 1446.2071842045325

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'F', 'L', 'Q', 'Y', 'L']:
****modeller loss --> 10 * 89.154539 = 891.54539
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -3.56532 + -0.1 * -1248.07739
****cdr loss --> 100 * 58.595445418469 = 5859.5445418469 given no cdr
****iedb loss --> 50 * 24.737986002516028 = 1236.8993001258013

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'M', 'L', 'Q', 'Y', 'A']:
****modeller loss --> 10 * 47.33086800000001 = 473.3086800000001
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -7.02512 + -0.1 * -1175.82068
****cdr loss --> 100 * 60.24973834918703 = 6024.973834918703 given no cdr
****iedb loss --> 50 * 27.530185005438778 = 1376.509250271939

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'D', 'L', 'Q', 'Y', 'T']:
****modeller loss --> 10 * 32.024946000000014 = 320.2494600000001
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -7.04794 + -0.1 * -1025.04346
****cdr loss --> 100 * 103.14044761911984 = 10314.044761911984 given no cdr
****iedb loss --> 50 * 28.29138157366404 = 1414.569078683202

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'Y', 'L', 'Q', 'Y', 'F']:
****modeller loss --> 10 * 102.84305600000002 = 1028.4305600000002
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -2.67817 + -0.1 * -1296.24756
****cdr loss --> 100 * 58.740639087490635 = 5874.063908749064 given no cdr
****iedb loss --> 50 * 28.370739295759233 = 1418.5369647879616

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'Y', 'L']:
****modeller loss --> 2.5 * -29.837807999999995 = -74.59451999999999
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -13.1685 + -0.1 * -1018.47192
****cdr loss --> 100 * 60.33432864295659 = 6033.432864295659 given no cdr
****iedb loss --> 50 * 24.50601453707825 = 1225.3007268539125

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'K', 'L', 'Q', 'Y', 'I']:
****modeller loss --> 2.5 * -14.615825000000001 = -36.5395625
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -12.54424 + -0.1 * -1108.26575
****cdr loss --> 100 * 60.18375578447722 = 6018.375578447722 given no cdr
****iedb loss --> 50 * 25.475672122566174 = 1273.7836061283087

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'I', 'L', 'Q', 'Y', 'I']:
****modeller loss --> 10 * 256.397526 = 2563.97526
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * 13.17908 + -0.1 * -1246.06726
****cdr loss --> 100 * 60.96924137445234 = 6096.924137445234 given no cdr
****iedb loss --> 50 * 25.771508784851104 = 1288.5754392425551

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'Q', 'L', 'Q', 'Y', 'H']:
****modeller loss --> 10 * 51.63920800000001 = 516.3920800000001
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -6.42214 + -0.1 * -1158.60608
****cdr loss --> 100 * 103.14044761911984 = 10314.044761911984 given no cdr
****iedb loss --> 50 * 30.41486554066744 = 1520.743277033372

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'Q', 'L', 'Q', 'Y', 'L']:
****modeller loss --> 10 * 132.817404 = 1328.17404
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * 2.16478 + -0.1 * -1111.69604
****cdr loss --> 100 * 103.14044761911984 = 10314.044761911984 given no cdr
****iedb loss --> 50 * 24.63880215874993 = 1231.9401079374964
Delta:  [4329.430603674851, -1128.3054104689627, -1241.5028772510223, 2932.568658153523, -795.2632089046383, -1932.1555712920926, -1860.6750203656338, 833.1801942461261, 3234.885476503692, 3757.8642674078164]
Accept 5 times
delta_decrease 5 times:
dict_values(['V', 'M', 'N', 'I', 'F', 'L', 'Q', 'Y', 'L'])
dict_values(['V', 'M', 'N', 'I', 'M', 'L', 'Q', 'Y', 'A'])
dict_values(['V', 'M', 'N', 'I', 'Y', 'L', 'Q', 'Y', 'F'])
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'Y', 'L'])
dict_values(['V', 'M', 'N', 'I', 'K', 'L', 'Q', 'Y', 'I'])
delta_notdecrease 0 times
Reject 5 times
dict_values(['V', 'M', 'N', 'I', 'P', 'L', 'Q', 'Y', 'Y'])
dict_values(['V', 'M', 'N', 'I', 'D', 'L', 'Q', 'Y', 'T'])
dict_values(['V', 'M', 'N', 'I', 'I', 'L', 'Q', 'Y', 'I'])
dict_values(['V', 'M', 'N', 'I', 'Q', 'L', 'Q', 'Y', 'H'])
dict_values(['V', 'M', 'N', 'I', 'Q', 'L', 'Q', 'Y', 'L'])
Step 00001: change accepted >> LOSS 9116.295 --> 7184.139
sequence --> target:  ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'Y', 'L']
======================================================================
[[0.09670933 0.04727767 0.09318748 0.06359184 0.07574431 0.05577483
  0.07373346 0.07539656 0.08572309]
 [0.05563581 0.02782904 0.03071733 0.03885578 0.04411243 0.03483082
  0.03975022 0.05933934 0.02301646]
 [0.03607488 0.02326393 0.04088249 0.04706901 0.03708678 0.04357512
  0.03828303 0.04518231 0.01911121]
 [0.02920283 0.0282059  0.06575365 0.10611515 0.0388198  0.03897642
  0.03821708 0.03277126 0.02102771]
 [0.01336861 0.01281424 0.0139649  0.01383446 0.01222209 0.01481741
  0.01393229 0.01372265 0.01162979]
 [0.03438036 0.03204644 0.04354369 0.04011967 0.03141759 0.03591302
  0.04418657 0.05779413 0.01279869]
 [0.0312741  0.02958772 0.03573232 0.12771499 0.05559995 0.04870632
  0.06506242 0.08554597 0.02316598]
 [0.06936174 0.03978939 0.0518643  0.07792412 0.07432429 0.04807226
  0.04065588 0.06370163 0.04118792]
 [0.02597335 0.01374937 0.02622025 0.02540035 0.03053119 0.02015019
  0.05140892 0.03268629 0.01052319]
 [0.06415993 0.07030919 0.07275026 0.04185489 0.06440263 0.10435839
  0.07044895 0.04111419 0.07186545]
 [0.08606066 0.39055614 0.13685261 0.06252577 0.13120094 0.14942132
  0.0923357  0.08309784 0.29340596]
 [0.08448639 0.02972072 0.03775668 0.05394507 0.08243364 0.04107821
  0.04591843 0.07359941 0.03324253]
 [0.01931519 0.04216062 0.0330858  0.01468927 0.02071901 0.01914748
  0.01929189 0.01802477 0.01920913]
 [0.06385644 0.02564252 0.05279709 0.02953239 0.06825098 0.03813204
  0.04944295 0.04158867 0.04115238]
 [0.02115105 0.02547882 0.03344957 0.06306384 0.02717052 0.05049979
  0.06891495 0.02737018 0.01601326]
 [0.0864455  0.03256961 0.06386095 0.06990772 0.04929611 0.0530485
  0.04890707 0.07507869 0.02578292]
 [0.05490157 0.03538698 0.04592924 0.04625999 0.04234299 0.0551112
  0.05606154 0.07437422 0.02372631]
 [0.00891878 0.00679915 0.02115674 0.00870449 0.00923337 0.00814546
  0.01490965 0.00824795 0.00587178]
 [0.04876444 0.02053375 0.04038375 0.0210229  0.03692311 0.02366428
  0.0385017  0.03857158 0.02037137]
 [0.06995904 0.0662788  0.0601109  0.04786829 0.06816828 0.11657694
  0.0900373  0.05279235 0.20117488]]
Now the best target step is 1
########### Now Step-2 ###########
########### Mutation Positions are [8, 7] ###########

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'G', 'Q']:
****modeller loss --> 2.5 * -33.16127300000001 = -82.90318250000001
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -11.47836 + -0.1 * -816.22327
****cdr loss --> 100 * 105.29083921805613 = 10529.083921805613 given no cdr
****iedb loss --> 50 * 29.70223464393783 = 1485.1117321968916

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'Q', 'V']:
****modeller loss --> 10 * 52.05187 = 520.5187
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -2.68991 + -0.1 * -789.5097
****cdr loss --> 100 * 61.905699675466856 = 6190.569967546686 given no cdr
****iedb loss --> 50 * 23.757528543324725 = 1187.8764271662362

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 2.5 * -97.79548899999998 = -244.48872249999994
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -19.08656 + -0.1 * -930.70111
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 24.48838080305496 = 1224.419040152748

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'Q', 'G']:
****modeller loss --> 2.5 * -17.458348 = -43.64587
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -9.49145 + -0.1 * -774.56152
****cdr loss --> 100 * 61.905699675466856 = 6190.569967546686 given no cdr
****iedb loss --> 50 * 29.136131104248605 = 1456.8065552124303

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'V', 'A']:
****modeller loss --> 2.5 * -27.145436000000004 = -67.86359000000002
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -12.58209 + -0.1 * -986.75464
****cdr loss --> 100 * 64.11945846442856 = 6411.945846442856 given no cdr
****iedb loss --> 50 * 26.25431249948789 = 1312.7156249743946

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'A', 'V']:
****modeller loss --> 10 * 42.323540000000015 = 423.23540000000014
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -5.40891 + -0.1 * -964.1264
****cdr loss --> 100 * 64.44467354652514 = 6444.467354652514 given no cdr
****iedb loss --> 50 * 23.817730811911574 = 1190.8865405955787

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'G', 'V']:
****modeller loss --> 10 * 85.67600200000001 = 856.7600200000002
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * 0.88581 + -0.1 * -768.17902
****cdr loss --> 100 * 105.29083921805613 = 10529.083921805613 given no cdr
****iedb loss --> 50 * 24.180910099010315 = 1209.0455049505158

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'L', 'V']:
****modeller loss --> 10 * 92.04757900000001 = 920.4757900000002
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -0.09546 + -0.1 * -930.02179
****cdr loss --> 100 * 63.279902587605946 = 6327.990258760595 given no cdr
****iedb loss --> 50 * 23.94704743622267 = 1197.3523718111335

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'K', 'M']:
****modeller loss --> 2.5 * -77.50219100000002 = -193.75547750000007
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -16.17893 + -0.1 * -842.87109
****cdr loss --> 100 * 105.29083921805613 = 10529.083921805613 given no cdr
****iedb loss --> 50 * 27.334064998305927 = 1366.7032499152963

LOSS RESULTS of the ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'T', 'I']:
****modeller loss --> 2.5 * -64.255771 = -160.63942749999998
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -15.83681 + -0.1 * -941.12329
****cdr loss --> 100 * 60.33704550859051 = 6033.704550859052 given no cdr
****iedb loss --> 50 * 25.120874541968416 = 1256.0437270984207
Delta:  [4747.153400352934, 714.8260235633497, -79.60871059937745, 419.59158160954394, 472.65881026767966, 874.4502240985212, 5410.750375606558, 1261.6793494221565, 4517.892623071337, -55.03022069209874]
Accept 2 times
delta_decrease 2 times:
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'T', 'I'])
delta_notdecrease 0 times
Reject 8 times
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'G', 'Q'])
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'Q', 'V'])
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'Q', 'G'])
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'V', 'A'])
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'A', 'V'])
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'G', 'V'])
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'L', 'V'])
dict_values(['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'K', 'M'])
Step 00002: change accepted >> LOSS 7184.139 --> 7104.530
sequence --> target:  ['V', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']
======================================================================
[[0.09670933 0.04727767 0.09318748 0.06359184 0.07574431 0.05577483
  0.07373346 0.06571135 0.07697207]
 [0.05563581 0.02782904 0.03071733 0.03885578 0.04411243 0.03483082
  0.03975022 0.05997988 0.02397433]
 [0.03607488 0.02326393 0.04088249 0.04706901 0.03708678 0.04357512
  0.03828303 0.04567003 0.01990656]
 [0.02920283 0.0282059  0.06575365 0.10611515 0.0388198  0.03897642
  0.03821708 0.03312501 0.02190281]
 [0.01336861 0.01281424 0.0139649  0.01383446 0.01222209 0.01481741
  0.01393229 0.01387078 0.01211379]
 [0.03438036 0.03204644 0.04354369 0.04011967 0.03141759 0.03591302
  0.04418657 0.04679293 0.01067465]
 [0.0312741  0.02958772 0.03573232 0.12771499 0.05559995 0.04870632
  0.06506242 0.0864694  0.02413007]
 [0.06936174 0.03978939 0.0518643  0.07792412 0.07432429 0.04807226
  0.04065588 0.05157593 0.03435249]
 [0.02597335 0.01374937 0.02622025 0.02540035 0.03053119 0.02015019
  0.05140892 0.04249702 0.01410336]
 [0.06415993 0.07030919 0.07275026 0.04185489 0.06440263 0.10435839
  0.07044895 0.05345453 0.0963153 ]
 [0.08606066 0.39055614 0.13685261 0.06252577 0.13120094 0.14942132
  0.0923357  0.07798367 0.28371377]
 [0.08448639 0.02972072 0.03775668 0.05394507 0.08243364 0.04107821
  0.04591843 0.06906981 0.03214441]
 [0.01931519 0.04216062 0.0330858  0.01468927 0.02071901 0.01914748
  0.01929189 0.01691546 0.01857459]
 [0.06385644 0.02564252 0.05279709 0.02953239 0.06825098 0.03813204
  0.04944295 0.0420376  0.04286501]
 [0.02115105 0.02547882 0.03344957 0.06306384 0.02717052 0.05049979
  0.06891495 0.02766563 0.01667968]
 [0.0864455  0.03256961 0.06386095 0.06990772 0.04929611 0.0530485
  0.04890707 0.07588913 0.02685592]
 [0.05490157 0.03538698 0.04592924 0.04625999 0.04234299 0.0551112
  0.05606154 0.0966975  0.0317984 ]
 [0.00891878 0.00679915 0.02115674 0.00870449 0.00923337 0.00814546
  0.01490965 0.00833698 0.00611614]
 [0.04876444 0.02053375 0.04038375 0.0210229  0.03692311 0.02366428
  0.0385017  0.03898794 0.02121916]
 [0.06995904 0.0662788  0.0601109  0.04786829 0.06816828 0.11657694
  0.0900373  0.04726943 0.18558748]]
Now the best target step is 2
########### Now Step-3 ###########
########### Mutation Positions are [8, 0] ###########

LOSS RESULTS of the ['G', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'A']:
****modeller loss --> 10 * 84.202773 = 842.0277299999999
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -0.21791 + -0.1 * -863.81873
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 26.109815871148466 = 1305.4907935574233

LOSS RESULTS of the ['L', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'L']:
****modeller loss --> 10 * 79.21901100000001 = 792.1901100000001
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -1.45274 + -0.1 * -937.46411
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 24.55325014928945 = 1227.6625074644726

LOSS RESULTS of the ['T', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'C']:
****modeller loss --> 10 * 106.496598 = 1064.96598
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * 1.40866 + -0.1 * -924.09998
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 29.462387033006785 = 1473.1193516503392

LOSS RESULTS of the ['S', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'I']:
****modeller loss --> 10 * 28.824998000000008 = 288.24998000000005
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -6.00962 + -0.1 * -889.21198
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 25.490742292714973 = 1274.5371146357486

LOSS RESULTS of the ['I', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'A']:
****modeller loss --> 10 * 52.329859000000006 = 523.2985900000001
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -4.20841 + -0.1 * -944.13959
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 26.176636607891457 = 1308.8318303945728

LOSS RESULTS of the ['F', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'M']:
****modeller loss --> 2.5 * -74.98937499999997 = -187.47343749999993
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -17.08459 + -0.1 * -958.56525
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 27.9745175174937 = 1398.725875874685

LOSS RESULTS of the ['T', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'A']:
****modeller loss --> 2.5 * -4.062310000000011 = -10.155775000000027
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -9.33294 + -0.1 * -892.6709
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 26.20162619500784 = 1310.081309750392

LOSS RESULTS of the ['G', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'A']:
****modeller loss --> 10 * 84.202773 = 842.0277299999999
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -0.21791 + -0.1 * -863.81873
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 26.109815871148466 = 1305.4907935574233

LOSS RESULTS of the ['M', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'G']:
****modeller loss --> 10 * 7.828309000000019 = 78.28309000000019
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -8.53234 + -0.1 * -931.51709
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 31.45864119256008 = 1572.932059628004

LOSS RESULTS of the ['L', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'D']:
****modeller loss --> 10 * 87.780601 = 877.80601
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -0.75571 + -0.1 * -953.37701
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 30.486317280016625 = 1524.3158640008312
Delta:  [1167.5882059046744, 1039.9222998117248, 1558.1550139975916, 582.8567769830006, 852.2001027418246, 231.3221207219367, 319.99521709764304, 1167.5882059046744, 671.2848319752557, 1422.1915563480834]
All is rejected.
dict_values(['G', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'A'])
dict_values(['L', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'L'])
dict_values(['T', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'C'])
dict_values(['S', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'I'])
dict_values(['I', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'A'])
dict_values(['F', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'M'])
dict_values(['T', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'A'])
dict_values(['G', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'A'])
dict_values(['M', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'G'])
dict_values(['L', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'D'])
Step 00003: change rejected >> LOSS 7104.530 !-> 7335.852
----------------------------------------------------------------------
[[0.07903405 0.04727767 0.09318748 0.06359184 0.07574431 0.05577483
  0.07373346 0.0660014  0.06470573]
 [0.06102164 0.02782904 0.03071733 0.03885578 0.04411243 0.03483082
  0.03975022 0.05996069 0.02692147]
 [0.03956712 0.02326393 0.04088249 0.04706901 0.03708678 0.04357512
  0.03828303 0.04565542 0.02235365]
 [0.02974422 0.0282059  0.06575365 0.10611515 0.0388198  0.03897642
  0.03821708 0.03311441 0.02283618]
 [0.01361645 0.01281424 0.0139649  0.01383446 0.01222209 0.01481741
  0.01393229 0.01386635 0.01263   ]
 [0.03770855 0.03204644 0.04354369 0.04011967 0.03141759 0.03591302
  0.04418657 0.0471224  0.01206326]
 [0.03430159 0.02958772 0.03573232 0.12771499 0.05559995 0.04870632
  0.06506242 0.08644174 0.02709636]
 [0.06098123 0.03978939 0.0518643  0.07792412 0.07432429 0.04807226
  0.04065588 0.05193907 0.03114942]
 [0.0284877  0.01374937 0.02622025 0.02540035 0.03053119 0.02015019
  0.05140892 0.04220321 0.01574673]
 [0.06070448 0.07030919 0.07275026 0.04185489 0.06440263 0.10435839
  0.07044895 0.05308496 0.09264732]
 [0.07566253 0.39055614 0.13685261 0.06252577 0.13120094 0.14942132
  0.0923357  0.07813683 0.25585957]
 [0.09266511 0.02972072 0.03775668 0.05394507 0.08243364 0.04107821
  0.04591843 0.06920547 0.03616725]
 [0.01827493 0.04216062 0.0330858  0.01468927 0.02071901 0.01914748
  0.01929189 0.01694868 0.01802742]
 [0.06504028 0.02564252 0.05279709 0.02953239 0.06825098 0.03813204
  0.04944295 0.04202415 0.04469167]
 [0.02319858 0.02547882 0.03344957 0.06306384 0.02717052 0.05049979
  0.06891495 0.02765678 0.0187301 ]
 [0.08804813 0.03256961 0.06386095 0.06990772 0.04929611 0.0530485
  0.04890707 0.07586485 0.02800036]
 [0.05194474 0.03538698 0.04592924 0.04625999 0.04234299 0.0551112
  0.05606154 0.09602896 0.03058742]
 [0.00978216 0.00679915 0.02115674 0.00870449 0.00923337 0.00814546
  0.01490965 0.00833432 0.006868  ]
 [0.05348509 0.02053375 0.04038375 0.0210229  0.03692311 0.02366428
  0.0385017  0.03897547 0.02382762]
 [0.07673143 0.0662788  0.0601109  0.04786829 0.06816828 0.11657694
  0.0900373  0.04743483 0.20909047]]
Now the best target step is 2
########### Now Step-4 ###########
########### Mutation Positions are [0] ###########

LOSS RESULTS of the ['L', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 10 * 151.39464800000002 = 1513.94648
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * 4.11133 + -0.1 * -1102.81348
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 24.399678926039048 = 1219.9839463019523

LOSS RESULTS of the ['F', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 2.5 * -79.95647599999998 = -199.89118999999994
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -17.18414 + -0.1 * -918.84924
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 24.30735800798167 = 1215.3679003990835

LOSS RESULTS of the ['S', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 10 * 96.205719 = 962.05719
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * 0.84599 + -0.1 * -877.45819
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 23.942386179183856 = 1197.1193089591927

LOSS RESULTS of the ['K', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 10 * 8.643072000000004 = 86.43072000000004
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -8.33029 + -0.1 * -919.45972
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 23.985176975890916 = 1199.2588487945459

LOSS RESULTS of the ['I', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 2.5 * -45.66021499999998 = -114.15053749999996
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -13.48611 + -0.1 * -892.00885
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 24.592027680982458 = 1229.601384049123

LOSS RESULTS of the ['I', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 2.5 * -45.66021499999998 = -114.15053749999996
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -13.48611 + -0.1 * -892.00885
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 24.592027680982458 = 1229.601384049123

LOSS RESULTS of the ['C', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 2.5 * -30.941504999999992 = -77.35376249999999
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -12.26149 + -0.1 * -916.73395
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 27.975338779805096 = 1398.7669389902549

LOSS RESULTS of the ['G', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 10 * 23.38676600000001 = 233.8676600000001
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -6.89467 + -0.1 * -923.33466
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 24.525206944239468 = 1226.2603472119733

LOSS RESULTS of the ['L', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 10 * 151.39464800000002 = 1513.94648
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * 4.11133 + -0.1 * -1102.81348
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 24.399678926039048 = 1219.9839463019523

LOSS RESULTS of the ['K', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V']:
****modeller loss --> 10 * 8.643072000000004 = 86.43072000000004
Notice, modeller loss = w1 * best_molpdf + w2 * best_DOPE --> = 10 * -8.33029 + -0.1 * -919.45972
****cdr loss --> 100 * 61.246000428974455 = 6124.600042897446 given no cdr
****iedb loss --> 50 * 23.985176975890916 = 1199.2588487945459
Delta:  [1754.0001086492048, 35.54639274633519, 1179.2461813064438, 305.7592511417979, 135.52052889637525, 135.52052889637525, 341.4828588375067, 480.1976895592252, 1754.0001086492048, 305.7592511417979]
All is rejected.
dict_values(['L', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['F', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['S', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['K', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['I', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['I', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['C', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['G', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['L', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
dict_values(['K', 'M', 'N', 'I', 'G', 'L', 'Q', 'H', 'V'])
Step 00004: change rejected >> LOSS 7104.530 !-> 7140.077
----------------------------------------------------------------------
[[0.08369359 0.04727767 0.09318748 0.06359184 0.07574431 0.05577483
  0.07373346 0.0662027  0.06539763]
 [0.06384443 0.02782904 0.03071733 0.03885578 0.04411243 0.03483082
  0.03975022 0.05994738 0.02678042]
 [0.04139745 0.02326393 0.04088249 0.04706901 0.03708678 0.04357512
  0.03828303 0.04564529 0.02223653]
 [0.03120499 0.0282059  0.06575365 0.10611515 0.0388198  0.03897642
  0.03821708 0.03310706 0.02277893]
 [0.01326986 0.01281424 0.0139649  0.01383446 0.01222209 0.01481741
  0.01393229 0.01386327 0.01259834]
 [0.03945291 0.03204644 0.04354369 0.04011967 0.03141759 0.03591302
  0.04418657 0.04735105 0.01205347]
 [0.03588835 0.02958772 0.03573232 0.12771499 0.05559995 0.04870632
  0.06506242 0.08642255 0.0269544 ]
 [0.05981538 0.03978939 0.0518643  0.07792412 0.07432429 0.04807226
  0.04065588 0.05219109 0.03143022]
 [0.02980551 0.01374937 0.02622025 0.02540035 0.03053119 0.02015019
  0.05140892 0.0419993  0.01560105]
 [0.05515804 0.07030919 0.07275026 0.04185489 0.06440263 0.10435839
  0.07044895 0.05282847 0.09225865]
 [0.06899738 0.39055614 0.13685261 0.06252577 0.13120094 0.14942132
  0.0923357  0.07824312 0.25720674]
 [0.08365083 0.02972072 0.03775668 0.05394507 0.08243364 0.04107821
  0.04591843 0.06929961 0.03602765]
 [0.01922832 0.04216062 0.0330858  0.01468927 0.02071901 0.01914748
  0.01929189 0.01697174 0.01806366]
 [0.06338477 0.02564252 0.05279709 0.02953239 0.06825098 0.03813204
  0.04944295 0.04201482 0.04457962]
 [0.02427172 0.02547882 0.03344957 0.06306384 0.02717052 0.05049979
  0.06891495 0.02765064 0.01863197]
 [0.08580698 0.03256961 0.06386095 0.06990772 0.04929611 0.0530485
  0.04890707 0.07584801 0.02793016]
 [0.05465465 0.03538698 0.04592924 0.04625999 0.04234299 0.0551112
  0.05606154 0.09556499 0.0304591 ]
 [0.01023467 0.00679915 0.02115674 0.00870449 0.00923337 0.00814546
  0.01490965 0.00833247 0.00683201]
 [0.05595925 0.02053375 0.04038375 0.0210229  0.03692311 0.02366428
  0.0385017  0.03896681 0.02370278]
 [0.08028094 0.0662788  0.0601109  0.04786829 0.06816828 0.11657694
  0.0900373  0.04754962 0.20847668]]
Now the best target step is 2
######################################################################
